{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_stats(dataset_dir):\n",
    "    sum_r = sum_g = sum_b = 0.0\n",
    "    sum_sq_r = sum_sq_g = sum_sq_b = 0.0\n",
    "    total_pixels = 0\n",
    "    processed_files = 0\n",
    "    problematic_files = []\n",
    "\n",
    "    total_images = 0\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            total_images += len([f for f in os.listdir(class_path) \n",
    "                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    with tqdm(total=total_images, desc=\"Processing Images\", \n",
    "             unit=\"img\", dynamic_ncols=True) as pbar:\n",
    "        \n",
    "        for class_dir in os.listdir(dataset_dir):\n",
    "            class_path = os.path.join(dataset_dir, class_dir)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                \n",
    "                if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    problematic_files.append(f\"{img_path} - Not an image\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        raise ValueError(\"OpenCV failed to read image\")\n",
    "                        \n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    if img.dtype != np.uint8:\n",
    "                        raise ValueError(f\"Unexpected dtype: {img.dtype}\")\n",
    "                        \n",
    "                    img = img.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    sum_r += np.sum(img[:, :, 0])\n",
    "                    sum_g += np.sum(img[:, :, 1])\n",
    "                    sum_b += np.sum(img[:, :, 2])\n",
    "                    \n",
    "                    sum_sq_r += np.sum(np.square(img[:, :, 0]))\n",
    "                    sum_sq_g += np.sum(np.square(img[:, :, 1]))\n",
    "                    sum_sq_b += np.sum(np.square(img[:, :, 2]))\n",
    "                    \n",
    "                    total_pixels += img.shape[0] * img.shape[1]\n",
    "                    processed_files += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    problematic_files.append(f\"{img_path} - {str(e)}\")\n",
    "                \n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({\n",
    "                        'Processed': processed_files,\n",
    "                        'Errors': len(problematic_files),\n",
    "                        'Current': os.path.basename(class_dir)\n",
    "                    })\n",
    "\n",
    "    if total_pixels == 0:\n",
    "        raise ValueError(\"No valid images processed - check dataset path or file formats\")\n",
    "\n",
    "    mean = [\n",
    "        round(sum_r / total_pixels, 5),\n",
    "        round(sum_g / total_pixels, 5),\n",
    "        round(sum_b / total_pixels, 5)\n",
    "    ]\n",
    "    \n",
    "    std = [\n",
    "        round(np.sqrt((sum_sq_r / total_pixels) - (mean[0] ** 2)), 5),\n",
    "        round(np.sqrt((sum_sq_g / total_pixels) - (mean[1] ** 2)), 5),\n",
    "        round(np.sqrt((sum_sq_b / total_pixels) - (mean[2] ** 2)), 5)\n",
    "    ]\n",
    "\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Total images attempted: {total_images}\")\n",
    "    print(f\"Successfully processed: {processed_files}\")\n",
    "    print(f\"Problematic files: {len(problematic_files)}\")\n",
    "    \n",
    "    if problematic_files:\n",
    "        print(\"\\nFirst 5 errors:\")\n",
    "        for error in problematic_files[:5]:\n",
    "            print(f\" - {error}\")\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'Dataset'\n",
    "mean, std = calculate_dataset_stats(dataset_dir)\n",
    "\n",
    "dataset_mean = [round(float(m), 5) for m in mean]\n",
    "dataset_std = [round(float(s), 5) for s in std]\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"Dataset Mean (RGB):\", dataset_mean)\n",
    "print(\"Dataset Std (RGB):\", dataset_std)\n",
    "\n",
    "assert all(0 <= m <= 1 for m in dataset_mean), \"Mean values out of [0,1] range\"\n",
    "assert all(0 <= s <= 1 for s in dataset_std), \"Std values out of [0,1] range\"\n",
    "\n",
    "assert abs(mean[0]*255 - 0.57119 * 255) < 0.1, \"Red mean mismatch\"\n",
    "assert abs(mean[1]*255 - 0.57445 * 255) < 0.1, \"Green mean mismatch\"\n",
    "assert abs(mean[2]*255 - 0.60537 * 255) < 0.1, \"Blue mean mismatch\"\n",
    "\n",
    "assert abs(std[0]*255 - 0.11762 * 255) < 0.1, \"Red std mismatch\"\n",
    "assert abs(std[1]*255 - 0.12342 * 255) < 0.1, \"Green std mismatch\"\n",
    "assert abs(std[2]*255 - 0.11999 * 255) < 0.1, \"Blue std mismatch\"\n",
    "\n",
    "print(\"\\nBasic validation checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = [0.57121, 0.57445, 0.60535]\n",
    "dataset_std = [0.11756, 0.12333, 0.11987]\n",
    "\n",
    "imageNet_mean = [0.485, 0.456, 0.406]\n",
    "imageNet_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_album = A.Compose(\n",
    "    [   \n",
    "        A.Resize(64, 64),\n",
    "        A.Affine(\n",
    "            translate_percent=0.02,\n",
    "            scale=(0.95, 1.05),\n",
    "            fill=128,\n",
    "            p=0.3\n",
    "        ),\n",
    "        A.Perspective(\n",
    "            scale=(0.05, 0.1),\n",
    "            keep_size=True,\n",
    "            fill=128,\n",
    "            p=0.4\n",
    "        ),\n",
    "        # A.HueSaturationValue(\n",
    "        #     hue_shift_limit=2,\n",
    "        #     sat_shift_limit=3,\n",
    "        #     val_shift_limit=3,\n",
    "        #     p=0.2\n",
    "        # ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=(0, 0.2),\n",
    "            contrast_limit=0.1,\n",
    "            p=0.2\n",
    "        ),\n",
    "        A.RGBShift(\n",
    "          r_shift_limit=2,\n",
    "          g_shift_limit=2,\n",
    "          b_shift_limit=2,\n",
    "          p=0.2 \n",
    "        ),\n",
    "        A.GaussianBlur(\n",
    "            blur_limit=(3, 7),\n",
    "            p=0.2\n",
    "        ),\n",
    "        A.GaussNoise(\n",
    "            std_range=(0.01, 0.02),\n",
    "            mean_range=(0.04, 0.06),\n",
    "            p=0.2\n",
    "        ),\n",
    "        # A.CoarseDropout(\n",
    "        #     num_holes_range=(1, 2),\n",
    "        #     hole_height_range=(4, 6),\n",
    "        #     hole_width_range=(4, 6),\n",
    "        #     fill=128,\n",
    "        #     p=0.2\n",
    "        # ),\n",
    "        A.Normalize(\n",
    "            mean=tuple(dataset_mean),\n",
    "            std=tuple(dataset_std)\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_simple = A.Compose(\n",
    "    [\n",
    "        A.Resize(64, 64),\n",
    "        A.Normalize(\n",
    "            mean=tuple(dataset_mean),\n",
    "            std=tuple(dataset_std)\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'Dataset'\n",
    "output_dir = r'Augmented_Dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "images_path_list = []\n",
    "labels_path_list = []\n",
    "class_idx = {}\n",
    "augmentation_count = 5\n",
    "\n",
    "class_names = sorted([d for d in os.listdir(root_dir)\n",
    "                      if os.path.isdir(os.path.join(root_dir, d))])\n",
    "\n",
    "for cls_idx, class_name in enumerate(class_names):\n",
    "    class_idx[cls_idx] = class_name\n",
    "    images_tmp = os.path.join(root_dir, class_name, \"*.png\")\n",
    "    images_paths = glob.glob(images_tmp)\n",
    "    \n",
    "    for img_path in images_paths:\n",
    "        images_path_list.append(img_path)\n",
    "        labels_path_list.append(cls_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'barred_area',\n",
       " 1: 'cross_walk',\n",
       " 2: 'go_straight',\n",
       " 3: 'no_passing_zone_beginning',\n",
       " 4: 'parking_zone',\n",
       " 5: 'priority_over',\n",
       " 6: 'steep_hill_downhill',\n",
       " 7: 'steep_hill_uphill',\n",
       " 8: 'stop',\n",
       " 9: 'tunnel_beginning',\n",
       " 10: 'turn_left',\n",
       " 11: 'turn_right'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Images: 100%|██████████| 33595/33595 [03:17<00:00, 170.05img/s, Processing: turn_right/frame_0573_aug_05.png]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation complete. 6719 → 33595 files\n",
      "Augmented images saved in: c:\\Users\\tahaa\\Documents\\PythonProjects\\MachineLearning\\IKIUSelfDrivingCar\\Augmented_Dataset\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(images_path_list) * augmentation_count, \n",
    "            desc=\"Augmenting Images\", \n",
    "            unit=\"img\", \n",
    "            dynamic_ncols=True)\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(class_dir):\n",
    "        raise RuntimeError(f\"Failed to create directory: {class_dir}\")\n",
    "\n",
    "for img_path in images_path_list:\n",
    "    class_name = os.path.basename(os.path.dirname(img_path))\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image {img_path}\")\n",
    "        pbar.update(augmentation_count)\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    for aug_num in range(1, augmentation_count + 1):\n",
    "        transformed = transform_album(image=img)\n",
    "        transformed_img = transformed['image']\n",
    "        \n",
    "        transformed_img = transformed_img.permute(1, 2, 0).numpy()\n",
    "        transformed_img = (transformed_img * np.array(dataset_std) + np.array(dataset_mean))\n",
    "        transformed_img = np.clip(transformed_img * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        new_filename = f\"{base_name}_aug_{aug_num:02d}.png\"\n",
    "        new_path = os.path.join(output_dir, class_name, new_filename)\n",
    "        \n",
    "        if not os.path.exists(os.path.dirname(new_path)):\n",
    "            os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            save_success = cv2.imwrite(new_path, cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR))\n",
    "            if not save_success:\n",
    "                Image.fromarray(transformed_img).save(new_path)\n",
    "                print(f\"Used PIL fallback for {new_path}\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"Failed to save {new_path}: {str(save_error)}\")\n",
    "            continue\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(f\"Processing: {class_name}/{new_filename}\")\n",
    "\n",
    "pbar.close()\n",
    "output_files = glob.glob(os.path.join(output_dir, \"**\", \"*.png\"), recursive=True)\n",
    "print(f\"\\nAugmentation complete. {len(images_path_list)} → {len(images_path_list)*augmentation_count} files\")\n",
    "print(f\"Augmented images saved in: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 6719/6719 [02:07<00:00, 52.65img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. 6719 images replaced\n",
      "Original images overwritten in: c:\\Users\\tahaa\\Documents\\PythonProjects\\MachineLearning\\IKIUSelfDrivingCar\\Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_path_list = []\n",
    "for class_name in os.listdir(root_dir):\n",
    "    class_path = os.path.join(root_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        images_path_list.extend(glob.glob(os.path.join(class_path, \"*.png\")))\n",
    "\n",
    "pbar = tqdm(total=len(images_path_list), \n",
    "           desc=\"Processing Images\", \n",
    "           unit=\"img\", \n",
    "           dynamic_ncols=True)\n",
    "\n",
    "for img_path in images_path_list:\n",
    "    try:\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transformed = transform_simple(image=img)\n",
    "        processed_img = transformed['image']\n",
    "        \n",
    "        processed_img = processed_img.permute(1, 2, 0).numpy()\n",
    "        processed_img = (processed_img * dataset_std + dataset_mean) * 255\n",
    "        processed_img = processed_img.clip(0, 255).astype(np.uint8)\n",
    "        \n",
    "        cv2.imwrite(img_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "        pbar.update(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {img_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nProcessing complete. {len(images_path_list)} images replaced\")\n",
    "print(f\"Original images overwritten in: {os.path.abspath(root_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignDB(Dataset):\n",
    "    def __init__(self, images_path_list, labels_path_list, transform=None):\n",
    "        self.images_path_list = images_path_list\n",
    "        self.labels_path_list = labels_path_list\n",
    "        self.transform = transform\n",
    "        self.idx_to_class = self._create_label_mapping()\n",
    "        \n",
    "    def _create_label_mapping(self):\n",
    "        return {idx: name for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.images_path_list[index]\n",
    "        label = self.labels_path_list[index]\n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                if isinstance(self.transform, A.Compose):\n",
    "                    # Albumentations Pipeline\n",
    "                    augmented = self.transform(image=image)\n",
    "                    image = augmented[\"image\"]\n",
    "                else:\n",
    "                    # PyTorch PIL\n",
    "                    image = Image.open(path).convert(\"RGB\")\n",
    "                    image = self.transform(image)\n",
    "            else:\n",
    "                image = ToTensorV2()(image=image)[\"image\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {path}: {e}\")\n",
    "            image = torch.zeros(3, 256, 256)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_name(self, label_idx):\n",
    "        return self.idx_to_class.get(label_idx, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSign_train = TrafficSignDB(\n",
    "    images_path_list=images_path_list,\n",
    "    labels_path_list=labels_path_list,\n",
    "    transform=transform_Simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_sample(dataset, index):\n",
    "    image, label = dataset[index]\n",
    "    class_name = dataset.get_class_name(label)\n",
    "    \n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        # mean = np.array(dataset_mean)\n",
    "        # std = np.array(dataset_std)\n",
    "        \n",
    "        mean = np.array(imageNet_mean)\n",
    "        std = np.array(imageNet_std)\n",
    "        \n",
    "        image = image * std + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Class: {class_name}\\nLabel: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #### Plot Multiple Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, len(trafficSign_train)-1)\n",
    "sign_sample(trafficSign_train, random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignCNN(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(TrafficSignCNN, self).__init__()\n",
    "        \n",
    "        # self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trafficSign_train, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TrafficSignCNN(num_classes=12)\n",
    "model = model.to(device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_pred += torch.sum(preds == labels).item()\n",
    "        total_pred += labels.size(0)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct_pred / total_pred\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"traffic_sign_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TrafficSignCNN(num_classes=12)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'barred_area',\n",
    "    'cross_walk',\n",
    "    'go_straight',\n",
    "    'no_passing_zone_beginning',\n",
    "    'parking_zone',\n",
    "    'priority_over',\n",
    "    'steep_hill_downhill',\n",
    "    'steep_hill_uphill',\n",
    "    'stop',\n",
    "    'tunnel_beginning',\n",
    "    'turn_left',\n",
    "    'turn_right'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_test(model_path):\n",
    "    # Load model\n",
    "    model = TrafficSignCNN(num_classes=12)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    # Define proper transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convert numpy array to PIL first\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imageNet_mean, std=imageNet_std)\n",
    "    ])\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # Convert to RGB and process\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        tensor_img = transform(rgb).unsqueeze(0).to(device)  # No 'image=' keyword\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tensor_img)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            sign_name = class_names[pred.item()]\n",
    "        \n",
    "        # Display\n",
    "        cv2.putText(frame, sign_name, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Detection', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'): break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_test(\"traffic_sign_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
